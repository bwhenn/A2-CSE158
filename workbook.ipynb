{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69de8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import datetime \n",
    "import struct\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Iterable, Tuple, List, Any\n",
    "\n",
    "IMAGE_FEATURES_DATA_PATH = \"data/Behance_Image_Features.b\"\n",
    "ITEMS_TO_OWNERS_DATA_PATH = \"data/Behance_Item_to_Owners.gz\"\n",
    "APPRECIATE_DATA_PATH = \"data/Behance_appreciate_1M.gz\"\n",
    "IMAGE_FEATURE_LIMIT = 70000  # set to None to load all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9049e36",
   "metadata": {},
   "source": [
    "# Assignment 2 â€“ Behance Like Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6bce9",
   "metadata": {},
   "source": [
    "## 1. Predictive Task & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab1d62",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0607b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6c5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gzipped_text_file(path: str) -> Iterable[Tuple[str, ...]]:\n",
    "    try:\n",
    "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                segments = tuple(line.strip().split())\n",
    "                yield segments\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def readImageFeatures(path: str):\n",
    "    f = open(path, \"rb\")\n",
    "    while True:\n",
    "        itemId = f.read(8)\n",
    "        if not itemId or len(itemId) < 8:\n",
    "            break\n",
    "        feature = struct.unpack(\"f\" * 4096, f.read(4 * 4096))\n",
    "        yield itemId, feature\n",
    "\n",
    "def _decode_item_id(raw_id):\n",
    "    if isinstance(raw_id, (bytes, bytearray)):\n",
    "        return raw_id.decode(\"utf-8\")\n",
    "    return str(raw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1055ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = readImageFeatures(path=IMAGE_FEATURES_DATA_PATH)\n",
    "g2 = process_gzipped_text_file(path=ITEMS_TO_OWNERS_DATA_PATH)\n",
    "g3 = process_gzipped_text_file(path=APPRECIATE_DATA_PATH)\n",
    "\n",
    "# Item ownership lookups\n",
    "item_to_owner: Dict[str, str] = {}\n",
    "owner_to_items: Dict[str, set] = defaultdict(set)\n",
    "for row in g2:\n",
    "    item, owner = row[0], row[1]\n",
    "    item_to_owner[item] = owner\n",
    "    owner_to_items[owner].add(item)\n",
    "\n",
    "# Interaction histories\n",
    "user_to_items: Dict[str, List[Tuple[str, Any]]] = defaultdict(list)  # user -> list of (item, timestamp)\n",
    "item_to_users: Dict[str, List[Tuple[str, Any]]] = defaultdict(list)  # item -> list of (user, timestamp)\n",
    "for user_id, item_id, ts in g3:\n",
    "    ts_int = int(ts) if ts.isdigit() else ts\n",
    "    user_to_items[user_id].append((item_id, ts_int))\n",
    "    item_to_users[item_id].append((user_id, ts_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c437b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_counter(counter: Counter) -> Dict[str, float]:\n",
    "    if not counter:\n",
    "        return {\"count\": 0, \"mean\": 0.0, \"median\": 0.0, \"p90\": 0.0, \"p99\": 0.0, \"max\": 0}\n",
    "    vals = np.fromiter(counter.values(), dtype=np.int64)\n",
    "    return {\n",
    "        \"count\": len(counter),\n",
    "        \"mean\": float(vals.mean()),\n",
    "        \"median\": float(np.percentile(vals, 50)),\n",
    "        \"p90\": float(np.percentile(vals, 90)),\n",
    "        \"p99\": float(np.percentile(vals, 99)),\n",
    "        \"max\": int(vals.max()),\n",
    "    }\n",
    "\n",
    "def load_feature_ids(path: str, limit: int = None) -> set:\n",
    "    ids = set()\n",
    "    for idx, (raw_id, _) in enumerate(readImageFeatures(path)):\n",
    "        ids.add(_decode_item_id(raw_id))\n",
    "        if limit and idx + 1 >= limit:\n",
    "            break\n",
    "    return ids\n",
    "\n",
    "def compute_eda(\n",
    "    ownership_path: str = ITEMS_TO_OWNERS_DATA_PATH,\n",
    "    interaction_path: str = APPRECIATE_DATA_PATH,\n",
    "    feature_path: str = IMAGE_FEATURES_DATA_PATH,\n",
    "    feature_id_limit: int = None,  \n",
    ") -> Dict[str, Any]:\n",
    "    owner_item_counts = Counter()\n",
    "    with gzip.open(ownership_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                owner_item_counts[parts[1]] += 1\n",
    "    ownership_summary = summarize_counter(owner_item_counts)\n",
    "\n",
    "    user_inter_counts = Counter()\n",
    "    item_inter_counts = Counter()\n",
    "    day_bins = Counter()\n",
    "    ts_min, ts_max = None, None\n",
    "    with gzip.open(interaction_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 3:\n",
    "                user, item, ts = parts[0], parts[1], int(parts[2])\n",
    "                user_inter_counts[user] += 1\n",
    "                item_inter_counts[item] += 1\n",
    "                day_bins[ts // 86400] += 1\n",
    "                ts_min = ts if ts_min is None else min(ts_min, ts)\n",
    "                ts_max = ts if ts_max is None else max(ts_max, ts)\n",
    "\n",
    "    cold_users_1 = sum(1 for v in user_inter_counts.values() if v == 1)\n",
    "    cold_items_1 = sum(1 for v in item_inter_counts.values() if v == 1)\n",
    "    cold_users_3 = sum(1 for v in user_inter_counts.values() if v <= 3)\n",
    "    cold_items_3 = sum(1 for v in item_inter_counts.values() if v <= 3)\n",
    "\n",
    "    interaction_summary = {\n",
    "        \"total_interactions\": int(sum(user_inter_counts.values())),\n",
    "        \"users\": summarize_counter(user_inter_counts),\n",
    "        \"items\": summarize_counter(item_inter_counts),\n",
    "        \"timestamp_range\": (ts_min, ts_max),\n",
    "        \"cold_start\": {\n",
    "            \"users_eq1\": cold_users_1,\n",
    "            \"items_eq1\": cold_items_1,\n",
    "            \"users_le3\": cold_users_3,\n",
    "            \"items_le3\": cold_items_3,\n",
    "        },\n",
    "        \"top_days\": day_bins.most_common(5),\n",
    "    }\n",
    "\n",
    "    feature_ids = load_feature_ids(feature_path, limit=feature_id_limit)\n",
    "    inter_items = set(item_inter_counts.keys())\n",
    "    missing_features = inter_items - feature_ids\n",
    "    record_size = 8 + 4096 * 4\n",
    "    feature_count_from_size = os.path.getsize(feature_path) // record_size\n",
    "\n",
    "    return {\n",
    "        \"ownership\": ownership_summary,\n",
    "        \"interactions\": interaction_summary,\n",
    "        \"image_feature_ids_loaded\": len(feature_ids),\n",
    "        \"image_feature_count_filesize\": int(feature_count_from_size),\n",
    "        \"interaction_items_missing_features\": len(missing_features),\n",
    "    }\n",
    "\n",
    "def print_eda_report(eda: Dict[str, Any]) -> None:\n",
    "    print(\"\\n=== Exploratory Data Analysis ===\")\n",
    "    own = eda[\"ownership\"]\n",
    "    print(\n",
    "        f\"Ownership: {int(own.get('count', 0)):,} owners, \"\n",
    "        f\"avg items/owner={own.get('mean', 0):.2f}, median={own.get('median', 0):.0f}, \"\n",
    "        f\"p90={own.get('p90', 0):.0f}, p99={own.get('p99', 0):.0f}, max={own.get('max', 0):,}\"\n",
    "    )\n",
    "\n",
    "    inter = eda[\"interactions\"]\n",
    "    print(f\"\\nInteractions: {inter['total_interactions']:,}\")\n",
    "    u = inter[\"users\"]\n",
    "    print(\n",
    "        f\"Users: {int(u.get('count', 0)):,}; \"\n",
    "        f\"avg={u.get('mean', 0):.2f}, median={u.get('median', 0):.0f}, \"\n",
    "        f\"p90={u.get('p90', 0):.0f}, p99={u.get('p99', 0):.0f}, max={u.get('max', 0):,}\"\n",
    "    )\n",
    "    it = inter[\"items\"]\n",
    "    print(\n",
    "        f\"Items: {int(it.get('count', 0)):,}; \"\n",
    "        f\"avg={it.get('mean', 0):.2f}, median={it.get('median', 0):.0f}, \"\n",
    "        f\"p90={it.get('p90', 0):.0f}, p99={it.get('p99', 0):.0f}, max={it.get('max', 0):,}\"\n",
    "    )\n",
    "    ts_min, ts_max = inter[\"timestamp_range\"]\n",
    "    print(f\"Timestamps span: {ts_min} to {ts_max} \"\n",
    "          f\"({datetime.datetime.fromtimestamp(ts_max, tz=datetime.timezone.utc).date()})\")\n",
    "\n",
    "    cold = inter[\"cold_start\"]\n",
    "    print(\n",
    "        f\"Cold-start: users with 1 interaction={cold['users_eq1']:,}, <=3={cold['users_le3']:,}; \"\n",
    "        f\"items with 1 interaction={cold['items_eq1']:,}, <=3={cold['items_le3']:,}\"\n",
    "    )\n",
    "\n",
    "    top_days = [\n",
    "        f\"{datetime.datetime.fromtimestamp(day * 86400, tz=datetime.timezone.utc).date()}: {count:,}\"\n",
    "        for day, count in inter[\"top_days\"]\n",
    "    ]\n",
    "    print(\"Top 5 days by interactions: \" + (\"; \".join(top_days) if top_days else \"n/a\"))\n",
    "\n",
    "    print(\n",
    "        f\"Image features loaded: {eda['image_feature_ids_loaded']:,} \"\n",
    "        f\"(from file size est: {eda['image_feature_count_filesize']:,}); \"\n",
    "        f\"interaction items missing features: {eda['interaction_items_missing_features']:,}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a46b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exploratory Data Analysis ===\n",
      "Ownership: 51,487 owners, avg items/owner=3.62, median=2, p90=8, p99=21, max=153\n",
      "\n",
      "Interactions: 1,000,000\n",
      "Users: 63,497; avg=15.75, median=4, p90=32, p99=197, max=2,260\n",
      "Items: 178,788; avg=5.59, median=2, p90=8, p99=67, max=1,793\n",
      "Timestamps span: 1307583271 to 1321254674 (2011-11-14)\n",
      "Cold-start: users with 1 interaction=15,938, <=3=29,875; items with 1 interaction=84,996, <=3=136,787\n",
      "Top 5 days by interactions: 2011-11-08: 9,862; 2011-11-02: 9,485; 2011-10-24: 9,482; 2011-11-09: 9,453; 2011-10-03: 9,340\n",
      "Image features loaded: 70,000 (from file size est: 178,787); interaction items missing features: 108,788\n"
     ]
    }
   ],
   "source": [
    "eda_stats = compute_eda(feature_id_limit=IMAGE_FEATURE_LIMIT)  \n",
    "print_eda_report(eda_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cc0a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6d4149",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65313bf",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcc132",
   "metadata": {},
   "source": [
    "## 5. Related Work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A2CSE158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
