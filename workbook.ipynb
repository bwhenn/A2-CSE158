{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9049e36",
   "metadata": {},
   "source": [
    "# Assignment 2 – Behance Like Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6bce9",
   "metadata": {},
   "source": [
    "## 1. Predictive Task & Evaluation\n",
    "\n",
    "### Task formulation (Context)\n",
    "\n",
    "Our goal is to **predict which Behance projects a user will “appreciate” (like)** in the future, based on their past implicit feedback.\n",
    "\n",
    "- **Input data:** triples of the form \\((u, i, t)\\), where:\n",
    "  - \\(u\\) is a user ID\n",
    "  - \\(i\\) is a project (item) ID\n",
    "  - \\(t\\) is a timestamp of when the user appreciated the project\n",
    "- **Prediction task:** for a given user \\(u\\), and a set of candidate projects, we want to produce a **ranking** so that items the user actually appreciates are ranked as highly as possible.\n",
    "- **Supervision type:** this is an **implicit-feedback recommendation** problem (we only see positive interactions, not explicit ratings or true negatives).\n",
    "\n",
    "In our experiments, we evaluate a scoring function \\(s(u, i)\\) on **one positive project** and **100 sampled negatives** for each user in the validation/test sets. The model’s job is to assign higher scores to the positive project than to the negatives.\n",
    "\n",
    "### Inputs, outputs, and what is optimized\n",
    "\n",
    "- **Model inputs:**\n",
    "  - A user index \\(u\\)\n",
    "  - A project index \\(i\\)\n",
    "  - Optionally, an image feature vector for project \\(i\\) (in the visual and hybrid models)\n",
    "- **Model output:**\n",
    "  - A **real-valued score** \\(s(u, i)\\). Higher scores mean “more likely that user \\(u\\) will appreciate project \\(i\\).”\n",
    "- **Optimization objective:**\n",
    "  - For the **matrix factorization (MF)** model, we optimize a **logistic loss** over positive and sampled negative user–item pairs, with \\(L_2\\) regularization on user and item embeddings.\n",
    "  - For the **visual and hybrid** models, parameters are not learned directly in this notebook; instead, we combine precomputed image features with learned MF scores and tune a combination weight (alpha) using validation performance.\n",
    "\n",
    "### Evaluation metrics\n",
    "\n",
    "We evaluate models using two ranking metrics computed over each user’s 1 positive + 100 sampled negatives:\n",
    "\n",
    "- **AUC (Area Under the ROC Curve):**\n",
    "  - Interpreted here as \\(P(s(u, i_{pos}) > s(u, i_{neg}))\\) averaged over negatives.\n",
    "  - Measures how often the model ranks the true positive above a random negative.\n",
    "- **Precision@10 (P@10):**\n",
    "  - For each user, we rank the 1 positive + 100 negatives, take the **top 10**, and check whether the positive is inside that top-10 set.\n",
    "  - This approximates a practical scenario where we show the user a **small recommendation list** and ask whether it contains something they actually appreciated.\n",
    "\n",
    "We report both metrics because:\n",
    "\n",
    "- **AUC** captures overall ranking quality across all 101 items.\n",
    "- **P@10** focuses on the **very top of the ranking**, which is often the most important region for a recommender system’s UI.\n",
    "\n",
    "These evaluation choices are consistent with our goal: **produce a ranked list where true future appreciations appear near the top.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab1d62",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427dd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core ---\n",
    "import os, math, random, struct\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Data ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- ML helpers ---\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Utils ---\n",
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcdc16",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b54c68",
   "metadata": {},
   "source": [
    "### Load appreciates file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62472497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "path = os.path.join(data_dir, \"Behance_appreciate_1M.gz\")\n",
    "\n",
    "triples_raw = []\n",
    "with open(path, \"rt\") as f:\n",
    "    for line in f:\n",
    "        u, i, t = line.strip().split()\n",
    "        triples_raw.append((u, i, int(t)))\n",
    "\n",
    "len(triples_raw), triples_raw[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4ea92",
   "metadata": {},
   "source": [
    "### Map users/items to integer indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx, item2idx = {}, {}\n",
    "idx2user, idx2item = [], []\n",
    "\n",
    "triples = []  # (u_idx, i_idx, timestamp)\n",
    "\n",
    "for u, i, t in triples_raw:\n",
    "    if u not in user2idx:\n",
    "        user2idx[u] = len(user2idx)\n",
    "        idx2user.append(u)\n",
    "    if i not in item2idx:\n",
    "        item2idx[i] = len(item2idx)\n",
    "        idx2item.append(i)\n",
    "    triples.append((user2idx[u], item2idx[i], t))\n",
    "\n",
    "num_users = len(user2idx)\n",
    "num_items = len(item2idx)\n",
    "num_users, num_items, len(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09e8c3",
   "metadata": {},
   "source": [
    "### Train / val / test split (per user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_user = defaultdict(list)\n",
    "for u, i, t in triples:\n",
    "    by_user[u].append((t, i))\n",
    "\n",
    "train_pos = []\n",
    "val_pos   = []\n",
    "test_pos  = []\n",
    "\n",
    "for u, lst in by_user.items():\n",
    "    lst.sort()  # sort by time\n",
    "    if len(lst) >= 3:\n",
    "        *train_items, val_item, test_item = lst\n",
    "        train_pos.extend((u, i) for (_, i) in train_items)\n",
    "        val_pos.append((u, val_item[1]))\n",
    "        test_pos.append((u, test_item[1]))\n",
    "    elif len(lst) == 2:\n",
    "        (t1, i1), (t2, i2) = lst\n",
    "        train_pos.append((u, i1))\n",
    "        test_pos.append((u, i2))\n",
    "    else:\n",
    "        train_pos.append((u, lst[0][1]))\n",
    "\n",
    "len(train_pos), len(val_pos), len(test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191d245",
   "metadata": {},
   "source": [
    "### Users' liked items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f106781",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pos_items = defaultdict(set)\n",
    "for u, i in train_pos + val_pos + test_pos:\n",
    "    user_pos_items[u].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91b55c",
   "metadata": {},
   "source": [
    "### Negative sampling + eval data builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative(u):\n",
    "    \"\"\"Sample an item that user u has NOT liked.\"\"\"\n",
    "    while True:\n",
    "        j = random.randrange(num_items)\n",
    "        if j not in user_pos_items[u]:\n",
    "            return j\n",
    "\n",
    "def build_eval_data(pos_pairs, num_neg=100):\n",
    "    \"\"\"\n",
    "    pos_pairs: list of (u, i_pos).\n",
    "    Returns: list of dicts: {\"u\": u, \"pos\": i_pos, \"negs\": [neg_items]}\n",
    "    \"\"\"\n",
    "    eval_data = []\n",
    "    for u, i_pos in pos_pairs:\n",
    "        negs = [sample_negative(u) for _ in range(num_neg)]\n",
    "        eval_data.append({\"u\": u, \"pos\": i_pos, \"negs\": negs})\n",
    "    return eval_data\n",
    "\n",
    "val_data  = build_eval_data(val_pos,  num_neg=100)\n",
    "test_data = build_eval_data(test_pos, num_neg=100)\n",
    "\n",
    "len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d6adb",
   "metadata": {},
   "source": [
    "### Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactions = len(triples)\n",
    "print(\"Users:\", num_users)\n",
    "print(\"Items:\", num_items)\n",
    "print(\"Interactions:\", num_interactions)\n",
    "print(\"Train/Val/Test:\", len(train_pos), len(val_pos), len(test_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102ad87",
   "metadata": {},
   "source": [
    "### Likes per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161856fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = Counter(u for u, _, _ in triples)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(list(user_counts.values()), bins=50)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Likes per user\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.title(\"Distribution of likes per user\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4519f",
   "metadata": {},
   "source": [
    "### Likes per item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts_full = Counter(i for _, i, _ in triples)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(list(item_counts_full.values()), bins=50)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Likes per item\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.title(\"Distribution of likes per item\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d4149",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modeling-markdown",
   "metadata": {},
   "source": [
    "### 3.1 Task as an ML problem (Context)\n",
    "\n",
    "We formulate Behance like prediction as a **personalized ranking problem**:\n",
    "\n",
    "- We have a set of users \\(\\mathcal{U}\\) and a set of projects \\(\\mathcal{I}\\).\n",
    "- Whenever a user appreciates a project, we observe a positive interaction \\((u, i, t)\\).\n",
    "- We treat each observed \\((u, i)\\) as a **positive example** and sample additional **unobserved items** for the same user as **implicit negatives**.\n",
    "- The model learns a scoring function \\(s(u, i)\\) such that \\(s(u, i_{pos}) > s(u, i_{neg})\\) for as many user–item pairs as possible.\n",
    "\n",
    "In our implementation:\n",
    "\n",
    "- The **matrix factorization (MF)** model learns:\n",
    "  - \\(P \\in \\mathbb{R}^{U \\times K}\\): user latent factors\n",
    "  - \\(Q \\in \\mathbb{R}^{I \\times K}\\): item latent factors\n",
    "  - Score: \\(s_{MF}(u, i) = P_u^\\top Q_i\\)\n",
    "- The **visual model** builds a visual profile per user by averaging image features of liked projects, and scores a candidate project via **cosine similarity**.\n",
    "- The **hybrid model** combines MF and visual scores linearly:  \n",
    "  \\[ s_{hyb}(u, i) = \\alpha\\, s_{MF}(u, i) + (1 - \\alpha)\\, s_{vis}(u, i). \\]\n",
    "\n",
    "We compare these against a **popularity baseline**, which ignores users and simply ranks items by how many times they were appreciated in the training data.\n",
    "\n",
    "### 3.2 Modeling approaches: advantages and disadvantages\n",
    "\n",
    "**Popularity baseline**\n",
    "\n",
    "- **Idea:** rank all projects by how many likes they have, and recommend the same top projects to everyone.\n",
    "- **Advantages:**\n",
    "  - Very simple and efficient (just a count lookup).\n",
    "  - Strong baseline when interactions are heavily concentrated on a few very popular items.\n",
    "- **Disadvantages:**\n",
    "  - Completely ignores individual user preferences.\n",
    "  - Cannot recommend long-tail or niche projects that are not globally popular.\n",
    "\n",
    "**Matrix factorization (MF)**\n",
    "\n",
    "- **Idea:** embed users and items into a shared **K-dimensional latent space**, and compute scores as dot products.\n",
    "- **Advantages:**\n",
    "  - Captures user–item interaction patterns (“people who liked X also liked Y”).\n",
    "  - Scales relatively well once embeddings are learned; scoring is just a dot product.\n",
    "  - Our implementation uses a **logistic loss with negative sampling**, which is natural for implicit feedback.\n",
    "- **Disadvantages / challenges:**\n",
    "  - Training is more complex and computationally expensive than the popularity baseline.\n",
    "  - Requires hyperparameter choices (latent dimension K, learning rate, regularization, number of epochs).\n",
    "  - Cold-start items with few interactions are still hard, since they have poorly learned embeddings.\n",
    "\n",
    "**Visual-only model**\n",
    "\n",
    "- **Idea:** use **image features** to represent projects. For each user, average the features of all projects they liked to form a **visual profile**, and then score new projects by cosine similarity to that profile.\n",
    "- **Advantages:**\n",
    "  - Uses **content information** (how projects look), which is very relevant in a visual platform like Behance.\n",
    "  - Can help in cold-start settings: even if a project is new, as long as we have an image feature vector, we can still compute a score.\n",
    "- **Disadvantages / challenges:**\n",
    "  - Ignores global interaction patterns between users and items.\n",
    "  - Averaging features is a simple heuristic; it may not capture multiple different styles a user likes.\n",
    "  - In our results, this model performs significantly worse than the interaction-based models on P@10.\n",
    "\n",
    "**Hybrid model (MF + Visual)**\n",
    "\n",
    "- **Idea:** combine MF and visual scores with a weight \\(\\alpha\\), chosen based on validation performance.\n",
    "- **Advantages:**\n",
    "  - Balances **interaction signals** and **visual content signals**.\n",
    "  - Can still rank visually similar items even if they have limited interactions, while leveraging MF where data is abundant.\n",
    "- **Disadvantages / challenges:**\n",
    "  - Requires tuning the combination weight \\(\\alpha\\).\n",
    "  - Our combination is a simple linear blend; more advanced methods (e.g., learning \\(\\alpha\\) per user or per item) could potentially perform better but would increase complexity.\n",
    "\n",
    "Overall, MF and the hybrid model are more expressive and personalized, while the popularity baseline sets a strong, simple reference point. The visual-only model explores the value of pure content information.\n",
    "\n",
    "### 3.3 Code walkthrough (Modeling code)\n",
    "\n",
    "Below we implement and train the different models:\n",
    "\n",
    "1. **Popularity baseline** (Section: *Baseline model – Item popularity*):\n",
    "   - We count how many times each item appears in the training set and define `pop_score(u, i)` to return this count.\n",
    "   - This is our **trivial but strong baseline**.\n",
    "\n",
    "2. **MF training data and model** (Sections: *Build MF training data*, *Initialize parameters*, *MF scoring function*, *Training loop*):\n",
    "   - `build_mf_training_data` constructs labeled pairs `(u, i, y)`\n",
    "     where `y = 1` for positive (observed) interactions and `y = 0` for sampled negatives.\n",
    "   - We initialize user and item embeddings `P` and `Q` with small random values.\n",
    "   - `mf_score(u, i)` computes the dot product between user and item embeddings.\n",
    "   - `train_mf` performs **stochastic gradient descent** on the logistic loss, with \\(L_2\\) regularization on `P` and `Q`.\n",
    "\n",
    "3. **Visual model** (Sections: *Load image features*, *Build user visual profiles*, *Visual-only scoring function*):\n",
    "   - We load precomputed 4096-dimensional image features for each project.\n",
    "   - For each user, we average the features of all liked projects to build `user_visual[u]`.\n",
    "   - `visual_score(u, i)` computes cosine similarity between the user’s visual profile and the project’s feature vector.\n",
    "\n",
    "4. **Hybrid model** (Section: *Hybrid model (MF + Visual)*):\n",
    "   - `make_hybrid_score(alpha)` returns a scoring function that linearly combines `mf_score` and `visual_score`.\n",
    "   - We later evaluate several values of `alpha` on the validation set to pick the best weight.\n",
    "\n",
    "In the next section, we will define an evaluation helper and compare these models quantitatively using AUC and P@10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d3246",
   "metadata": {},
   "source": [
    "### Baseline model – Item popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = Counter(i for (u, i) in train_pos)\n",
    "\n",
    "def pop_score(u, i):\n",
    "    # same for all users; uses only item popularity\n",
    "    return item_counts[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01127a5e",
   "metadata": {},
   "source": [
    "### Build MF training data (with negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mf_training_data(num_neg_per_pos=2):\n",
    "    data = []\n",
    "    for u, i in train_pos:\n",
    "        data.append((u, i, 1))\n",
    "        for _ in range(num_neg_per_pos):\n",
    "            j = sample_negative(u)\n",
    "            data.append((u, j, 0))\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "mf_train_data = build_mf_training_data(num_neg_per_pos=2)\n",
    "len(mf_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5b033",
   "metadata": {},
   "source": [
    "### MF model: parameters and scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40        # latent dimension\n",
    "lr = 0.05\n",
    "reg = 0.001\n",
    "epochs = 5    # start small, increase if training fast\n",
    "\n",
    "P = 0.01 * np.random.randn(num_users, K)\n",
    "Q = 0.01 * np.random.randn(num_items, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_score(u, i):\n",
    "    return float(P[u] @ Q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1764ee",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mf(train_data, epochs=5, lr=0.05, reg=0.001):\n",
    "    global P, Q\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for u, i, y in train_data:\n",
    "            pred = P[u] @ Q[i]\n",
    "            p_hat = 1.0 / (1.0 + math.exp(-pred))  # logistic\n",
    "            grad = p_hat - y                       # d/dpred logloss\n",
    "\n",
    "            Pu = P[u]\n",
    "            Qi = Q[i]\n",
    "\n",
    "            P[u] -= lr * (grad * Qi + reg * Pu)\n",
    "            Q[i] -= lr * (grad * Pu + reg * Qi)\n",
    "\n",
    "            total_loss += -(y * math.log(p_hat + 1e-8) +\n",
    "                            (1 - y) * math.log(1 - p_hat + 1e-8))\n",
    "\n",
    "        avg_loss = total_loss / len(train_data)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, avg log-loss: {avg_loss:.4f}\")\n",
    "\n",
    "train_mf(mf_train_data, epochs=epochs, lr=lr, reg=reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d7bcc",
   "metadata": {},
   "source": [
    "### Load image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = os.path.join(data_dir, \"Behance_Image_Features.b\")\n",
    "print(\"Image file exists:\", os.path.exists(IMG_PATH))\n",
    "\n",
    "item_features = {}  # item_idx -> np.array(4096,)\n",
    "\n",
    "with open(IMG_PATH, \"rb\") as f:\n",
    "    while True:\n",
    "        item_id_bytes = f.read(8)\n",
    "        if not item_id_bytes:\n",
    "            break\n",
    "        raw_id = item_id_bytes.decode(\"ascii\").strip()\n",
    "        vec = f.read(4 * 4096)\n",
    "        if len(vec) < 4 * 4096:\n",
    "            break\n",
    "        feat = np.frombuffer(vec, dtype=np.float32)\n",
    "\n",
    "        # map original item ID -> our index\n",
    "        if raw_id in item2idx:\n",
    "            idx = item2idx[raw_id]\n",
    "            item_features[idx] = feat\n",
    "\n",
    "len(item_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047cf7e",
   "metadata": {},
   "source": [
    "### Build user visual profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_visual = {}\n",
    "\n",
    "for u in range(num_users):\n",
    "    liked = [i for (uu, i) in train_pos if uu == u and i in item_features]\n",
    "    if not liked:\n",
    "        continue\n",
    "    mat = np.stack([item_features[i] for i in liked])\n",
    "    user_visual[u] = mat.mean(axis=0)\n",
    "\n",
    "len(user_visual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454af938",
   "metadata": {},
   "source": [
    "### Visual-only scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_score(u, i):\n",
    "    vu = user_visual.get(u)\n",
    "    fi = item_features.get(i)\n",
    "    if vu is None or fi is None:\n",
    "        return 0.0\n",
    "    denom = (norm(vu) * norm(fi)) + 1e-8\n",
    "    return float(vu @ fi / denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d8c75",
   "metadata": {},
   "source": [
    "### Hybrid model (MF + Visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hybrid_score(alpha):\n",
    "    def score(u, i):\n",
    "        return alpha * mf_score(u, i) + (1 - alpha) * visual_score(u, i)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65313bf",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-context",
   "metadata": {},
   "source": [
    "### 4.1 Evaluation setup (Context)\n",
    "\n",
    "For each user in the validation and test sets, we construct one evaluation instance:\n",
    "\n",
    "- A **single positive project** (the held-out appreciated item for that user), and\n",
    "- **100 negative projects**, sampled uniformly from items the user has never appreciated.\n",
    "\n",
    "Given a scoring function `score_fn(u, i)`, we compute:\n",
    "\n",
    "- **AUC:** fraction of negatives whose score is below the positive’s score.\n",
    "- **P@10:** whether the positive item appears in the **top 10** ranked among 1 positive + 100 negatives.\n",
    "\n",
    "We then average these metrics over all users in the validation/test sets.\n",
    "\n",
    "This protocol lets us compare different models **under the same candidate set**, and focuses on whether the true appreciated project is ranked near the top of a plausible recommendation list.\n",
    "\n",
    "### 4.2 Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d60bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(score_fn, eval_data, k=10):\n",
    "    \"\"\"\n",
    "    score_fn(u, i) -> float\n",
    "    eval_data: list of {\"u\": u, \"pos\": i_pos, \"negs\": [j1,...]}\n",
    "    Returns: (mean AUC, mean Precision@k)\n",
    "    \"\"\"\n",
    "    aucs = []\n",
    "    precisions = []\n",
    "\n",
    "    for row in eval_data:\n",
    "        u   = row[\"u\"]\n",
    "        pos = row[\"pos\"]\n",
    "        negs = row[\"negs\"]\n",
    "\n",
    "        items = [pos] + negs\n",
    "        scores = np.array([score_fn(u, it) for it in items])\n",
    "\n",
    "        pos_score = scores[0]\n",
    "        neg_scores = scores[1:]\n",
    "\n",
    "        # AUC = P(score_pos > score_neg)\n",
    "        auc = np.mean(pos_score > neg_scores)\n",
    "        aucs.append(auc)\n",
    "\n",
    "        # Precision@k\n",
    "        order = np.argsort(-scores)  # descending\n",
    "        topk = order[:k]\n",
    "        prec = 1.0 if 0 in topk else 0.0\n",
    "        precisions.append(prec)\n",
    "\n",
    "    return float(np.mean(aucs)), float(np.mean(precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-section",
   "metadata": {},
   "source": [
    "### 4.3 Validation performance and model selection\n",
    "\n",
    "We first evaluate all models on the **validation set** to understand their behavior and to choose the best \\(\\alpha\\) for the hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "pop_auc_val, pop_prec_val = eval_model(pop_score, val_data, k=10)\n",
    "print(\"Popularity baseline (val): AUC =\", pop_auc_val, \"P@10 =\", pop_prec_val)\n",
    "\n",
    "# MF\n",
    "mf_auc_val, mf_prec_val = eval_model(mf_score, val_data, k=10)\n",
    "print(\"MF (val): AUC =\", mf_auc_val, \"P@10 =\", mf_prec_val)\n",
    "\n",
    "# Visual-only\n",
    "vis_auc_val, vis_prec_val = eval_model(visual_score, val_data, k=10)\n",
    "print(\"Visual-only (val): AUC =\", vis_auc_val, \"P@10 =\", vis_prec_val)\n",
    "\n",
    "# Hybrid with different alphas\n",
    "alphas = [0.2, 0.5, 0.8]\n",
    "hyb_val_results = []\n",
    "for alpha in alphas:\n",
    "    hyb_score = make_hybrid_score(alpha)\n",
    "    auc, prec = eval_model(hyb_score, val_data, k=10)\n",
    "    hyb_val_results.append((alpha, auc, prec))\n",
    "    print(f\"Hybrid alpha={alpha}: AUC={auc:.4f}, P@10={prec:.4f}\")\n",
    "\n",
    "hyb_val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-discussion",
   "metadata": {},
   "source": [
    "From the validation results:\n",
    "\n",
    "- The **popularity baseline** achieves relatively high P@10, confirming that many likes are concentrated on a few very popular projects.\n",
    "- The **MF model** improves AUC over popularity, meaning it ranks positives above negatives more consistently, but its P@10 is slightly lower than pure popularity.\n",
    "- The **visual-only model** has substantially lower performance, indicating that using image features alone is not sufficient in this setting.\n",
    "- The **hybrid models** with different \\(\\alpha\\) values improve AUC further, and for some \\(\\alpha\\) they also get closer to popularity in terms of P@10.\n",
    "\n",
    "Based on these trends, we choose **\\(\\alpha = 0.5\\)** as a reasonable compromise between MF and visual information for the final test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_best = 0.5\n",
    "hybrid_best = make_hybrid_score(alpha_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-section",
   "metadata": {},
   "source": [
    "### 4.4 Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc92684",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_auc_test, pop_prec_test = eval_model(pop_score, test_data, k=10)\n",
    "mf_auc_test,  mf_prec_test  = eval_model(mf_score,  test_data, k=10)\n",
    "vis_auc_test, vis_prec_test = eval_model(visual_score, test_data, k=10)\n",
    "hyb_auc_test, hyb_prec_test = eval_model(hybrid_best, test_data, k=10)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Popularity (baseline)\",\n",
    "        \"MF\",\n",
    "        \"Visual-only\",\n",
    "        f\"Hybrid (alpha={alpha_best})\"\n",
    "    ],\n",
    "    \"AUC\":  [pop_auc_test, mf_auc_test, vis_auc_test, hyb_auc_test],\n",
    "    \"P@10\": [pop_prec_test, mf_prec_test, vis_prec_test, hyb_prec_test],\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-discussion",
   "metadata": {},
   "source": [
    "The test results summarize how each model performs on held-out users and projects:\n",
    "\n",
    "- **Popularity (baseline):**\n",
    "  - Strong P@10, reflecting that recommending globally popular items is often enough to hit at least one item the user will appreciate in the top 10.\n",
    "- **MF:**\n",
    "  - Higher AUC than popularity, indicating better **overall ranking quality**, but a slightly lower P@10.\n",
    "- **Visual-only:**\n",
    "  - Much lower AUC and P@10, confirming that content alone is not competitive here.\n",
    "- **Hybrid (alpha = 0.5):\n",
    "  - Achieves the **best AUC** among the learned models, and P@10 that is between MF and popularity.\n",
    "\n",
    "Overall, our experiments show that:\n",
    "\n",
    "- The **popularity baseline remains difficult to beat at P@10**, which is consistent with heavy popularity skew in real-world platforms.\n",
    "- Our **MF and hybrid models** provide better global ranking quality (AUC), and the hybrid makes use of visual information without completely sacrificing top-10 performance.\n",
    "\n",
    "This suggests that future work might focus on **better integrating popularity into the learned models**, for example via popularity-aware regularization or explicit popularity features, to close the P@10 gap while maintaining strong AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcc132",
   "metadata": {},
   "source": [
    "## 5. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c192921",
   "metadata": {},
   "source": [
    "Our project fits into a long line of research on systems that suggest items to users based on\n",
    "their past behavior, such as likes or clicks, and on using image information to make better\n",
    "recommendations.\n",
    "\n",
    "**Using past likes and clicks to make recommendations:**  \n",
    "Many existing systems only see whether a user clicked or liked something, not a detailed\n",
    "rating. Early work such as *Collaborative Filtering for Implicit Feedback Datasets* (Hu,\n",
    "Koren, & Volinsky, 2008) and *BPR: Bayesian Personalized Ranking from Implicit\n",
    "Feedback* (Rendle, Freudenthaler, Gantner, & Schmidt-Thieme, 2009) showed how to use\n",
    "this kind of “yes/no” data to learn which users and items go well together. Their models\n",
    "learn a small list of numbers for each user and for each item, and then give a higher score\n",
    "to user–item pairs whose numbers line up well. At prediction time, the system picks the\n",
    "items with the highest scores for each user. Our main interaction based model is inspired by this\n",
    "same idea, where for each user and each project, we learn a short numeric description, and to\n",
    "recommend projects to a user we score all projects and keep the ones with the highest\n",
    "scores.\n",
    "\n",
    "**Popularity based recommendations:**  \n",
    "Other work on sites like YouTube, Pinterest, or Behance-style platforms has found that\n",
    "simply recommending the most popular items, meaning the ones that receive the most likes overall,\n",
    "can already work surprisingly well. Because many users interact with only a few items, and\n",
    "because attention is often focused on a small set of very popular items, “recommend what\n",
    "is globally popular” is commonly used as a simple starting point or a backup strategy. Our\n",
    "popularity based model follows this idea, since we rank projects by how many likes they\n",
    "have, and recommend the top ones to everyone. In our results, this approach is quite effective: \n",
    "on average, for each user, a large fraction of the top 10 projects we recommend\n",
    "this way are projects they actually liked in the held out test data. This supports the claim\n",
    "from prior work that likes tend to be heavily concentrated on a small set of very popular\n",
    "projects.\n",
    "\n",
    "**Using image information to make recommendations:**  \n",
    "More recent research combines interaction data with information extracted from the images\n",
    "themselves. In domains like fashion, art, and design, how something looks is very important,\n",
    "so using image information can help. Models such as VBPR (*VBPR: Visual Bayesian\n",
    "Personalized Ranking for Personalized Recommendation of Visual Content*; He &\n",
    "McAuley, 2016) use an image recognition network to turn each image into a long vector of\n",
    "numbers that captures aspects like style, color, and composition. They then combine this\n",
    "image based description with the interaction based model so that the system can\n",
    "recommend items that both match a user’s taste and look similar to things they have liked\n",
    "before, and can still recommend reasonable items even when there are few past likes for a\n",
    "given item. Other work on image popularity prediction, such as *What Makes an Image\n",
    "Popular?* (Khosla, Das Sarma, & Hamid, 2014), also shows that these kinds of image\n",
    "features are strongly related to whether an image will attract attention and engagement.\n",
    "\n",
    "Our visual only model is closest to this line of work on image popularity. In our case, we\n",
    "ignore user IDs and only look at image-based features, since we build a visual profile for each\n",
    "user by averaging the image features of the projects they liked, and then recommend\n",
    "projects whose image features are similar to that profile. We find that this image only\n",
    "approach does contain useful information, but it is not enough by itself: compared to our\n",
    "interaction-based model and our popularity based model, it suggests fewer projects that\n",
    "the user actually likes near the top of the ranked list.\n",
    "\n",
    "**Combining interaction and image information:**  \n",
    "A common idea in related works is to combine these different signals rather than choose\n",
    "only one of them. Many papers use a weighted combination of a score based on past\n",
    "interactions and a score based on content, such as image features. This lets the system\n",
    "balance between “people who liked X also liked Y” and “this looks similar to what you\n",
    "liked before,” and can help especially for users or items that do not have much interaction\n",
    "history. Our combined model follows this pattern, as for each user–project pair, we compute\n",
    "one score from the interaction based model and one from the image based model, and\n",
    "then take a weighted average of the two, controlled by a parameter that decides how much\n",
    "weight to give to each source of information. In line with prior work, we see that using a\n",
    "middle value for this weight, rather than relying purely on interactions or purely on images,\n",
    "gives the best overall ranking of projects for most users on our test data. However, our\n",
    "results also show that the pure popularity based model still gives the largest number of\n",
    "correct hits in the top 10 recommendations. This suggests that our simple way of combining\n",
    "signals does not yet fully exploit popularity information, a limitation that is also discussed in\n",
    "recent work on how to correctly handle very popular items when building recommendation\n",
    "systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
